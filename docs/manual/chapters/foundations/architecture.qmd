# Technical Architecture {#sec-architecture}

## System Design

### The Segfault Problem and Hybrid Solution

**Historical Context:** Initial versions used R's DuckDB package for all operations, resulting in a 50% pipeline failure rate due to segmentation faults. These crashes occurred unpredictably during database writes, losing hours of processing and providing no debugging information.

**Solution:** Separate concerns by language strength:
- **R:** Statistical transformations, factor handling, domain logic
- **Python:** Database operations, bulk I/O, error handling
- **Feather:** Type-safe data exchange between languages

**Result:** 100% pipeline reliability, 3x faster I/O, rich error context

### Architecture Diagram

```{mermaid}
flowchart TB
    subgraph Input["Data Sources"]
        RC1["REDCap 7679<br/>Main NE25"]
        RC2["REDCap 7943<br/>ECDI"]
        RC3["REDCap 7999<br/>CAHMI"]
        RC4["REDCap 8014<br/>Psychosocial"]
    end

    subgraph R["R Processing Layer"]
        EXT["REDCapR Extraction"]
        DICT["Dictionary Parsing"]
        TRANS["recode_it() Transformations"]
        FEATH["Feather Export"]
    end

    subgraph Python["Python Processing Layer"]
        LOAD["Chunked Data Loading"]
        INSERT["DuckDB Insert"]
        META["Metadata Generation"]
        DOC["Documentation Export"]
    end

    subgraph Storage["Data Storage"]
        DB[("DuckDB Database<br/>kidsights_local.duckdb")]
        TEMP["Temporary Feather Files<br/>tempdir()/ne25_pipeline/"]
    end

    subgraph Output["Generated Outputs"]
        HTML["Interactive HTML Docs"]
        JSON["JSON Data Dictionary"]
        QUARTO["Quarto Documentation"]
    end

    RC1 & RC2 & RC3 & RC4 --> EXT
    EXT --> DICT
    DICT --> TRANS
    TRANS --> FEATH
    FEATH --> TEMP
    TEMP --> LOAD
    LOAD --> INSERT
    INSERT --> DB
    DB --> META
    META --> DOC
    DOC --> HTML & JSON & QUARTO
```

## Directory Structure

### Project Organization

```
Kidsights-Data-Platform/
├── R/                          # R transformation functions
│   ├── extract/                # REDCap data extraction
│   ├── transform/              # Transformation logic
│   │   └── ne25_transforms.R   # Main recode_it() function
│   ├── harmonize/              # Cross-study harmonization
│   └── utils/                  # Helper functions
├── python/                     # Python utilities
│   ├── db/                     # Database operations
│   │   ├── connection.py       # DatabaseManager class
│   │   └── operations.py       # DatabaseOperations class
│   └── utils/                  # Logging, R execution
├── pipelines/                  # Executable pipelines
│   ├── orchestration/          # R pipeline controllers
│   │   └── ne25_pipeline.R     # Main NE25 pipeline
│   └── python/                 # Python pipeline scripts
│       ├── init_database.py    # Database initialization
│       ├── insert_raw_data.py  # Data insertion
│       └── generate_metadata.py # Metadata generation
├── config/                     # YAML configurations
│   ├── sources/                # Data source configs
│   │   └── ne25.yaml           # NE25 project specification
│   ├── duckdb.yaml             # Database configuration
│   └── derived_variables.yaml  # Derived variable definitions
├── codebook/                   # JSON metadata system
│   └── data/                   # Codebook data files
│       └── codebook.json       # Master codebook (305 items)
├── scripts/                    # Utility scripts
│   ├── temp/                   # Temporary R scripts
│   ├── documentation/          # Doc generation scripts
│   └── audit/                  # Data validation scripts
├── data/                       # Data storage
│   └── duckdb/                 # DuckDB databases
│       └── kidsights_local.duckdb # Local database (47MB)
└── docs/                       # Generated documentation
    ├── manual/                 # This manual
    └── data_dictionary/        # Variable documentation
```

## Component Details

### R Transformation Engine

**File:** `R/transform/ne25_transforms.R`

**Key Functions:**

```r
# Master transformation orchestrator
recode_it(dat, dict, my_API = NULL, what = "all")
  ├── Calls recode__() for each transformation type
  ├── Joins results back to original data
  └── Returns complete transformed dataset

# Internal transformation dispatcher
recode__(dat, dict, my_API = NULL, what = NULL,
         relevel_it = TRUE, add_labels = TRUE)
  ├── Implements domain-specific logic
  ├── Creates factor variables with proper ordering
  ├── Adds variable labels using labelled package
  └── Returns transformed subset
```

**Design Patterns:**

1. **Dictionary-Driven:** Uses REDCap data dictionary to get value labels
2. **Explicit Namespacing:** All function calls use `package::function()` format
3. **Error Handling:** Try-catch blocks with informative error messages
4. **Validation:** Checks for required variables before transformation
5. **Documentation:** Variable labels added using labelled::var_label()

**Package Dependencies:**
```r
required_packages <- c(
  "plyr",       # mapvalues (load BEFORE dplyr)
  "dplyr",      # Data manipulation
  "tidyr",      # Data reshaping
  "stringr",    # String operations
  "labelled"    # Variable labels
)
```

### Python Database Layer

**File:** `python/db/connection.py`

**DatabaseManager Class:**
```python
class DatabaseManager:
    """Manages DuckDB database connections with configuration"""

    def __init__(self, config_path: str)
        # Load config from YAML
        # Set database path and parameters

    def get_connection(self) -> duckdb.DuckDBPyConnection
        # Return context-managed connection
        # Ensures proper cleanup

    def test_connection(self) -> bool
        # Verify database accessible
        # Return success/failure
```

**File:** `python/db/operations.py`

**DatabaseOperations Class:**
```python
class DatabaseOperations:
    """High-level database operations"""

    def insert_dataframe(self, df, table_name, if_exists="append",
                        chunk_size=1000)
        # Chunked insertion for large datasets
        # Handles data type mapping
        # Transaction management

    def query_to_dataframe(self, query: str) -> pd.DataFrame
        # Execute query and return DataFrame
        # Proper type conversion

    def table_exists(self, table_name: str) -> bool
        # Check table existence

    def get_table_count(self, table_name: str) -> int
        # Get row count efficiently
```

### Feather File Exchange

**Why Feather?**
- **Speed:** 3x faster than CSV for R↔Python exchange
- **Type Safety:** Perfect preservation of R factors as pandas categories
- **Compression:** Efficient binary format
- **No Parsing:** Direct memory mapping

**R Export:**
```r
library(arrow)
arrow::write_feather(transformed_data, "temp/ne25_transformed.feather")
```

**Python Import:**
```python
import pandas as pd
df = pd.read_feather("temp/ne25_transformed.feather")
# R factors become pandas categoricals automatically
```

**Data Type Mappings:**

| R Type | Arrow Type | Python Type |
|--------|------------|-------------|
| factor | dictionary | category |
| numeric | double | float64 |
| integer | int32 | int32 |
| logical | bool | bool |
| character | string | object |

### Metadata Generation System

**File:** `pipelines/python/generate_metadata.py`

**Core Functions:**

```python
def analyze_variable(series: pd.Series, var_name: str) -> Dict[str, Any]:
    """Generate comprehensive metadata for a single variable"""
    # Detect data type (numeric, factor, logical)
    # Calculate summary statistics
    # Generate factor levels if categorical
    # Return metadata dictionary

def analyze_factor_variable(series: pd.Series, var_name: str) -> Dict[str, Any]:
    """Specialized analysis for factor variables"""
    # Extract unique levels
    # Count frequencies and percentages
    # Sort levels logically (education: low→high, etc.)
    # Identify reference level
    # Return enhanced factor metadata

def generate_metadata_from_table(table_name: str, db_ops: DatabaseOperations,
                                 exclude_columns: List[str] = None,
                                 derived_variables_list: Optional[List[str]] = None) -> List[Dict]:
    """Generate metadata for all variables in a table"""
    # Query data from DuckDB
    # Filter to derived variables only (if specified)
    # Analyze each variable
    # Categorize by domain (eligibility, race, education, etc.)
    # Return list of metadata dictionaries
```

**Intelligent Features:**

1. **Automatic Factor Detection:** Variables with ≤20 unique values treated as factors
2. **Smart Ordering:** Education levels, poverty categories, age ranges sorted logically
3. **Reference Level Selection:** Most common non-missing value becomes reference
4. **Domain Categorization:** Variables auto-categorized by name patterns
5. **Missing Data Analysis:** Separate handling of NA, "Don't Know", "Refuse"

### Configuration System

**Database Configuration** (`config/duckdb.yaml`):
```yaml
database:
  path: "data/duckdb/kidsights_local.duckdb"
  settings:
    threads: 4
    memory_limit: "4GB"
    temp_directory: "temp/duckdb"
```

**Data Source Configuration** (`config/sources/ne25.yaml`):
```yaml
study:
  name: "ne25"
  description: "Nebraska 2025 Longitudinal Study"

redcap_projects:
  - pid: 7679
    description: "Main NE25 Demographics and Development"
  - pid: 7943
    description: "ECDI Developmental Screening"
  - pid: 7999
    description: "CAHMI Health Survey"
  - pid: 8014
    description: "Psychosocial Measures"

transformations:
  enabled: true
  functions: ["include", "race", "education", "sex", "age", "income", "caregiver relationship"]
```

**Derived Variables** (`config/derived_variables.yaml`):
```yaml
all_derived_variables:
  # Eligibility (3 variables)
  - eligible
  - authentic
  - include

  # Race/Ethnicity (6 variables)
  - hisp
  - race
  - raceG
  - a1_hisp
  - a1_race
  - a1_raceG

  # Education (12 variables)
  - educ_max
  - educ_a1
  # ... etc
```

## Data Flow Details

### Pipeline Execution Sequence

**1. Database Initialization**
```bash
python pipelines/python/init_database.py --config config/sources/ne25.yaml
```
- Creates DuckDB database if not exists
- Defines table schemas
- Sets up indexes and constraints

**2. Data Extraction (R)**
```r
source("pipelines/orchestration/ne25_pipeline.R")
```
- Connects to REDCap via API
- Downloads data from 4 projects
- Parses REDCap data dictionaries
- Combines into single dataset

**3. Transformation (R)**
```r
transformed_data <- recode_it(raw_data, dict, what = "all")
```
- Applies 7 transformation categories
- Creates 21 derived variables
- Adds variable labels
- Validates output

**4. Feather Export (R)**
```r
arrow::write_feather(transformed_data, "temp/ne25_transformed.feather")
```
- Exports to binary format
- Preserves all data types
- Includes metadata

**5. Database Insertion (Python)**
```bash
python pipelines/python/insert_raw_data.py \
  --source temp/ne25_transformed.feather \
  --table ne25_transformed \
  --mode replace
```
- Reads Feather file
- Chunks data (1000 rows/batch)
- Inserts into DuckDB
- Verifies row counts

**6. Metadata Generation (Python)**
```bash
python pipelines/python/generate_metadata.py \
  --source-table ne25_transformed \
  --derived-only
```
- Analyzes 21 derived variables
- Generates factor metadata
- Exports to Feather and DuckDB

**7. Documentation Generation**
```bash
python scripts/documentation/generate_html_documentation.py
```
- Queries metadata from DuckDB
- Generates interactive HTML
- Creates JSON search index

### Error Handling Strategy

**R Layer:**
- Try-catch blocks around each transformation
- Informative error messages with variable names
- Graceful degradation (skip failed transformations)
- Debug data saved to temp directory

**Python Layer:**
- Structured logging with context
- Transaction rollback on errors
- Detailed error metadata (type, message, traceback)
- Connection pooling for reliability

**Feather Layer:**
- File existence checks
- Schema validation
- Automatic cleanup of temp files

## Performance Characteristics

### Current Benchmarks (3,906 records)

| Operation | Time | Notes |
|-----------|------|-------|
| REDCap Extraction | 45s | 4 API calls |
| R Transformations | 12s | All 21 variables |
| Feather Export | 1s | 47 variables |
| Python Import | 0.3s | From Feather |
| DuckDB Insertion | 2s | Chunked |
| Metadata Generation | 8s | 21 variables |
| **Total Pipeline** | **68s** | End-to-end |

### Scalability Considerations

**Current Scale:**
- 3,906 participants
- 1,880 raw variables
- 21 derived variables
- 47MB database

**Expected Scale (5 years):**
- ~10,000 participants
- ~2,500 raw variables
- ~50 derived variables
- ~150MB database

**Design Choices for Scale:**
- Chunked processing (1000 rows/batch)
- Indexed primary keys (pid, record_id)
- Efficient data types (int32 vs int64)
- Selective metadata generation (derived only)

## Security and Privacy

### Data Protection

**Local-Only Storage:**
- DuckDB database stored locally, not in cloud
- No external database connections
- Temp files in secure system directories

**API Credentials:**
- Stored outside project directory
- CSV format for flexibility
- Environment variable mapping
- Never committed to version control

**PHI Handling:**
- No PHI in derived variables
- Date fields not included in exports
- Geographic data limited to county level
- Age in years/months, not birthdates

### Access Control

**File System Permissions:**
- Database files: Read/write owner only
- Config files: Read-only after setup
- Temp directory: Auto-cleanup after pipeline

**Version Control:**
- .gitignore excludes data files
- .gitignore excludes credentials
- Only code and documentation committed

## Maintenance and Monitoring

### Health Checks

**Database Health:**
```python
from python.db.connection import DatabaseManager
dm = DatabaseManager()
print(dm.test_connection())  # True if healthy
```

**Pipeline Status:**
- Log files in `logs/` directory
- Structured logging with timestamps
- Success/failure exit codes
- Email notifications (planned)

### Backup Strategy

**Database Backups:**
- Daily backup to network drive
- Weekly backup to external storage
- Backup includes metadata tables

**Code Backups:**
- Git repository with remote
- Tagged releases for milestones
- Branch protection on main

---

**Next:** @sec-standards covers naming conventions and coding standards for implementing transformations.
