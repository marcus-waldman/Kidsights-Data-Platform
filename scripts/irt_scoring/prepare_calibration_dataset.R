# =============================================================================
# Prepare Calibration Dataset for Mplus IRT Recalibration
# =============================================================================
# Purpose: Interactive workflow to create complete calibration dataset combining:
#   - Historical data (NE20, NE22, USA24) from KidsightsPublic package
#   - Current NE25 data from ne25_transformed table
#   - NSCH national benchmarking data (2021, 2022)
#
# Outputs:
#   1. Mplus .dat file (tab-delimited, missing as ".")
#   2. Mplus .inp file (generated by MplusAutomation)
#   3. Mplus syntax Excel file: mplus/kidsights_syntax_FIXED.xlsx
#   4. DuckDB table: calibration_dataset_2020_2025
#   5. Quality flags CSV: docs/irt_scoring/quality_flags.csv
#   6. Item response cross-tabulation: docs/irt_scoring/item_response_crosstab.csv
#
# Usage:
#   source("scripts/irt_scoring/prepare_calibration_dataset.R")
#   prepare_calibration_dataset()
#
# Interactive prompts will guide through:
#   - NSCH sample size selection
#   - Output file path specification
# =============================================================================

#' Prepare Calibration Dataset for Mplus IRT Recalibration
#'
#' Interactive workflow to create complete Mplus calibration dataset combining
#' historical Nebraska studies (NE20, NE22, USA24), current NE25 data, and
#' national NSCH benchmarking samples (2021, 2022).
#'
#' @param codebook_path Character. Path to codebook.json file.
#'   Default: "codebook/data/codebook.json"
#' @param db_path Character. Path to DuckDB database file.
#'   Default: "data/duckdb/kidsights_local.duckdb"
#'
#' @return Invisible NULL. Creates files and database tables as side effects:
#'   - Mplus .dat file (tab-delimited, missing as ".")
#'   - Mplus .inp file (generated by MplusAutomation)
#'   - Mplus syntax Excel file: mplus/kidsights_syntax_FIXED.xlsx
#'     (MODEL, CONSTRAINT, PRIOR sheets)
#'   - DuckDB table: calibration_dataset_2020_2025
#'   - Quality flags CSV: docs/irt_scoring/quality_flags.csv
#'     (Data quality issues report)
#'   - Item response cross-tabulation CSV: docs/irt_scoring/item_response_crosstab.csv
#'     (Items × response values, counts in cells)
#'
#' @details
#' This interactive workflow performs the following operations:
#'
#' 1. **Load Historical Data**: Import NE20, NE22, USA24 from
#'    historical_calibration_2020_2024 table (41,577 records)
#' 2. **Load NE25 Data**: Extract current study data from ne25_transformed table,
#'    filtered by eligible=TRUE (3,507 records)
#' 3. **Load NSCH Data**: Sample national benchmarking data from NSCH 2021/2022
#'    (user-specified sample size, default 1000 per year)
#' 4. **Harmonize Variables**: Map all items to lex_equate naming convention
#'    using codebook.json lexicon system
#' 5. **Create Study Indicators**: Assign numeric codes (1=NE20, 2=NE22, 3=NE25,
#'    5=NSCH21, 6=NSCH22, 7=USA24)
#' 6. **Combine Data**: Stack all 6 studies with consistent structure
#' 7. **Prepare Mplus Data & Quality Validation**: Create mplus_data frame and
#'    validate data quality before writing to disk
#' 8. **Export to Mplus**: Write space-delimited .dat file with missing as "."
#' 9. **Store in DuckDB**: Create calibration_dataset_2020_2025 table with indexes
#' 10. **Generate MODEL Syntax**: Auto-generate Mplus syntax Excel file
#'
#' Interactive prompts guide through:
#' - NSCH sample size selection (default: 1000 per year)
#' - Output file path specification (default: mplus/calibdat.dat)
#'
#' Expected output dimensions:
#' - Records: ~47,000 (varies by NSCH sample size)
#' - Columns: 419 (studynum, id, years, 416 items)
#' - File size: ~38-40 MB
#' - Execution time: ~30 seconds
#'
#' @examples
#' \dontrun{
#' # Interactive mode (recommended)
#' source("scripts/irt_scoring/prepare_calibration_dataset.R")
#' prepare_calibration_dataset()
#'
#' # Non-interactive mode (command line)
#' "C:\Program Files\R\R-4.5.1\bin\R.exe" --slave --no-restore \
#'   --file=scripts/irt_scoring/prepare_calibration_dataset.R
#'
#' # Query resulting database table
#' library(duckdb)
#' conn <- dbConnect(duckdb(), "data/duckdb/kidsights_local.duckdb")
#' DBI::dbGetQuery(conn, "
#'   SELECT studynum, COUNT(*) as n
#'   FROM calibration_dataset_2020_2025
#'   GROUP BY studynum
#' ")
#' DBI::dbDisconnect(conn)
#' }
#'
#' @seealso
#' - \code{\link{recode_nsch_2021}} for NSCH 2021 harmonization
#' - \code{\link{recode_nsch_2022}} for NSCH 2022 harmonization
#' - \code{scripts/irt_scoring/import_historical_calibration.R} for one-time
#'   historical data import
#'
#' @export
prepare_calibration_dataset <- function(
    codebook_path = "codebook/data/codebook.json",
    db_path = "data/duckdb/kidsights_local.duckdb") {

  # ===========================================================================
  # Load Dependencies
  # ===========================================================================

  cat("\n")
  cat(strrep("=", 80), "\n")
  cat("PREPARE CALIBRATION DATASET FOR MPLUS IRT RECALIBRATION\n")
  cat(strrep("=", 80), "\n\n")

  cat("[SETUP] Loading required packages and helper functions\n")

  required_packages <- c("duckdb", "dplyr", "jsonlite", "stringr", "MplusAutomation")

  for (pkg in required_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      stop(sprintf("Package '%s' is required but not installed.\n", pkg),
           sprintf("Install with: install.packages('%s')", pkg))
    }
  }

  library(duckdb)
  library(dplyr)
  library(jsonlite)
  library(stringr)
  library(MplusAutomation)

  # Source helper functions
  source("scripts/irt_scoring/helpers/recode_nsch_2021.R")
  source("scripts/irt_scoring/helpers/recode_nsch_2022.R")
  source("R/codebook/load_codebook.R")

  cat("        Packages and functions loaded\n\n")

  # Load codebook for item ordering
  cat("[SETUP] Loading codebook for canonical item ordering\n")
  codebook <- load_codebook("codebook/data/codebook.json", validate = FALSE)
  cat(sprintf("        Codebook loaded: %d items\n", codebook$metadata$total_items))

  # Extract calibration items from codebook (calibration_item = true)
  cat("[SETUP] Extracting calibration items from codebook\n")
  codebook_json <- jsonlite::fromJSON(codebook_path, simplifyVector = FALSE)
  calibration_items <- character(0)

  for (item_id in names(codebook_json$items)) {
    item <- codebook_json$items[[item_id]]

    # Check if item is marked for calibration
    is_calibration_item <- item$psychometric$calibration_item

    if (!is.null(is_calibration_item) && is_calibration_item == TRUE) {
      # Get equate lexicon name
      equate_name <- item$lexicons$equate
      if (!is.null(equate_name) && nchar(equate_name) > 0) {
        calibration_items <- c(calibration_items, equate_name)
      }
    }
  }

  cat(sprintf("        Calibration items: %d items (calibration_item = TRUE)\n\n", length(calibration_items)))

  # ===========================================================================
  # Step 1: Load Historical Data (NE20, NE22, USA24)
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 1: LOAD HISTORICAL CALIBRATION DATA\n")
  cat(strrep("=", 80), "\n\n")

  cat("[1/10] Loading historical calibration data from DuckDB\n")

  # Connect to database
  if (!file.exists(db_path)) {
    stop(sprintf("DuckDB database not found at: %s", db_path))
  }

  conn <- duckdb::dbConnect(duckdb::duckdb(), dbdir = db_path, read_only = TRUE)

  # Check if historical table exists
  tables <- DBI::dbGetQuery(conn, "SHOW TABLES")
  if (!"historical_calibration_2020_2024" %in% tables$name) {
    DBI::dbDisconnect(conn)
    stop("Table 'historical_calibration_2020_2024' not found.\n",
         "Run scripts/irt_scoring/import_historical_calibration.R first.")
  }

  # Load historical data
  historical_data <- DBI::dbGetQuery(conn,
    "SELECT * FROM historical_calibration_2020_2024")
  DBI::dbDisconnect(conn)

  # Display record counts by study
  study_counts <- table(historical_data$study)
  cat(sprintf("        Loaded %d records from historical_calibration_2020_2024\n",
              nrow(historical_data)))
  cat("\n        Study breakdown:\n")
  for (study_name in names(study_counts)) {
    cat(sprintf("          %s: %d records\n", study_name, study_counts[study_name]))
  }

  cat(sprintf("\n        Items: %d columns\n", ncol(historical_data) - 3))
  cat("\n")

  # ===========================================================================
  # Step 2: Load NE25 Data
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 2: LOAD NE25 DATA\n")
  cat(strrep("=", 80), "\n\n")

  cat("[2/10] Loading NE25 data from ne25_transformed table\n")

  # Load codebook for item mappings
  if (!file.exists(codebook_path)) {
    stop(sprintf("Codebook not found at: %s", codebook_path))
  }
  codebook <- jsonlite::fromJSON(codebook_path, simplifyVector = FALSE)

  # Build mapping: ne25 lexicon (lowercase) -> lex_equate (uppercase)
  ne25_to_equate <- list()
  for (item_id in names(codebook$items)) {
    item <- codebook$items[[item_id]]
    if (!is.null(item$lexicons$ne25) && nchar(item$lexicons$ne25) > 0) {
      ne25_name <- tolower(item$lexicons$ne25)  # Database has lowercase
      equate_name <- item$lexicons$equate
      ne25_to_equate[[ne25_name]] <- equate_name
    }
  }

  cat(sprintf("        Found %d items with ne25 lexicon mappings\n", length(ne25_to_equate)))

  # Query NE25 data
  conn <- duckdb::dbConnect(duckdb::duckdb(), dbdir = db_path, read_only = TRUE)

  if (!"ne25_transformed" %in% DBI::dbGetQuery(conn, "SHOW TABLES")$name) {
    DBI::dbDisconnect(conn)
    stop("Table 'ne25_transformed' not found in database")
  }

  ne25_raw <- DBI::dbGetQuery(conn, "
    SELECT *
    FROM ne25_transformed
    WHERE eligible = TRUE AND authentic = TRUE
  ")
  DBI::dbDisconnect(conn)

  cat(sprintf("        Loaded %d eligible and authentic records\n", nrow(ne25_raw)))

  # Extract and rename items
  ne25_item_cols <- intersect(tolower(names(ne25_raw)), names(ne25_to_equate))

  # Create rename mapping for dplyr (new_name = old_name)
  rename_map <- character()
  for (ne25_col in ne25_item_cols) {
    equate_name <- ne25_to_equate[[ne25_col]]
    rename_map[equate_name] <- ne25_col
  }

  cat(sprintf("        Mapping %d items to lex_equate names\n", length(rename_map)))

  # Build NE25 dataset
  ne25_data <- ne25_raw %>%
    dplyr::select(pid, record_id, years_old, dplyr::any_of(ne25_item_cols)) %>%
    dplyr::rename(!!!rename_map) %>%  # Rename to lex_equate
    dplyr::mutate(
      id = as.numeric(paste0("312025", pid, stringr::str_pad(record_id, 5, "left", "0"))),
      study = "NE25",
      years = years_old
    ) %>%
    dplyr::select(-pid, -record_id, -years_old) %>%
    dplyr::relocate(id, study, years)

  cat(sprintf("        NE25 data prepared: %d records, %d items\n",
              nrow(ne25_data), ncol(ne25_data) - 3))
  cat("\n")

  # ===========================================================================
  # STEPS 3-5 SKIPPED: NSCH Data Excluded Due to Quality Concerns
  # ===========================================================================
  #
  # NSCH 2021 and 2022 data have been excluded from calibration due to
  # systematic negative age correlations detected in quality validation.
  #
  # Issue: 26 of 30 NSCH 2021 items show negative correlations with age,
  # including strong magnitudes (|r| > 0.5) for ability items like:
  #   - TELLSTORY (r = -0.659)
  #   - WRITENAME (r = -0.649)
  #   - ASKQUESTION2 (r = -0.574)
  #
  # Investigation confirmed correct reverse coding implementation, suggesting
  # fundamental measurement artifacts in NSCH "how often" response scales.
  #
  # See: .github/ISSUE_TEMPLATE/nsch_calibration_data_exclusion.md
  #
  # To re-enable NSCH data:
  #   1. Uncomment Steps 3-5 below
  #   2. Add nsch21_data and nsch22_data to bind_rows() in Step 6
  #   3. Re-add NSCH21 (5) and NSCH22 (6) to studynum case_when
  #
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("[STEPS 3-5 SKIPPED: NSCH data excluded - see issue documentation]\n")
  cat(strrep("=", 80), "\n\n")

  # # ===========================================================================
  # # Step 3: NSCH Sample Size Prompt [DISABLED]
  # # ===========================================================================
  #
  # cat(strrep("=", 80), "\n")
  # cat("STEP 3: NSCH SAMPLE SIZE SELECTION\n")
  # cat(strrep("=", 80), "\n\n")
  #
  # cat("[3/10] Select NSCH sample size\n\n")
  # cat("NSCH data will be sampled to reduce file size and speed up Mplus execution.\n")
  # cat("Recommended: 1000 records per year (total: 2000 records)\n")
  # cat("Enter sample size per year, or 'all' for complete datasets\n\n")
  #
  # nsch_sample_size <- readline(prompt = "NSCH sample size per year (default: 1000): ")
  #
  # if (nsch_sample_size == "" || is.na(nsch_sample_size)) {
  #   nsch_sample_size <- 1000
  #   cat(sprintf("        Using default: %d records per year\n\n", nsch_sample_size))
  # } else if (tolower(nsch_sample_size) %in% c("all", "inf")) {
  #   nsch_sample_size <- Inf
  #   cat("        Using all available records\n\n")
  # } else {
  #   nsch_sample_size <- as.numeric(nsch_sample_size)
  #   if (is.na(nsch_sample_size) || nsch_sample_size <= 0) {
  #     stop("Invalid sample size. Must be positive number or 'all'")
  #   }
  #   cat(sprintf("        Using %d records per year\n\n", nsch_sample_size))
  # }
  #
  # # ===========================================================================
  # # Step 4: Load NSCH 2021 Data [DISABLED]
  # # ===========================================================================
  #
  # cat(strrep("=", 80), "\n")
  # cat("STEP 4: LOAD NSCH 2021 DATA\n")
  # cat(strrep("=", 80), "\n\n")
  #
  # cat("[4/10] Loading NSCH 2021 data\n\n")
  #
  # # Call recode function (includes all output messages)
  # nsch21_full <- recode_nsch_2021(codebook_path = codebook_path, db_path = db_path)
  #
  # # Apply sampling if needed
  # if (is.finite(nsch_sample_size) && nrow(nsch21_full) > nsch_sample_size) {
  #   set.seed(2021)  # Reproducible sampling
  #   nsch21_data <- nsch21_full %>% dplyr::slice_sample(n = nsch_sample_size)
  #   cat(sprintf("[INFO] Sampled %d of %d NSCH 2021 records\n\n",
  #               nrow(nsch21_data), nrow(nsch21_full)))
  # } else {
  #   nsch21_data <- nsch21_full
  #   cat(sprintf("[INFO] Using all %d NSCH 2021 records\n\n", nrow(nsch21_data)))
  # }
  #
  # # Add study identifier (recode function doesn't create this)
  # nsch21_data <- nsch21_data %>% dplyr::mutate(study = "NSCH21")
  #
  # # ===========================================================================
  # # Step 5: Load NSCH 2022 Data [DISABLED]
  # # ===========================================================================
  #
  # cat(strrep("=", 80), "\n")
  # cat("STEP 5: LOAD NSCH 2022 DATA\n")
  # cat(strrep("=", 80), "\n\n")
  #
  # cat("[5/10] Loading NSCH 2022 data\n\n")
  #
  # # Call recode function (includes all output messages)
  # nsch22_full <- recode_nsch_2022(codebook_path = codebook_path, db_path = db_path)
  #
  # # Apply sampling if needed
  # if (is.finite(nsch_sample_size) && nrow(nsch22_full) > nsch_sample_size) {
  #   set.seed(2022)  # Reproducible sampling
  #   nsch22_data <- nsch22_full %>% dplyr::slice_sample(n = nsch_sample_size)
  #   cat(sprintf("[INFO] Sampled %d of %d NSCH 2022 records\n\n",
  #               nrow(nsch22_data), nrow(nsch22_full)))
  # } else {
  #   nsch22_data <- nsch22_full
  #   cat(sprintf("[INFO] Using all %d NSCH 2022 records\n\n", nrow(nsch22_data)))
  # }
  #
  # # Add study identifier (recode function doesn't create this)
  # nsch22_data <- nsch22_data %>% dplyr::mutate(study = "NSCH22")

  # ===========================================================================
  # Step 3: Combine Datasets & Create Study Indicator (was Step 6)
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 3: COMBINE DATASETS & CREATE STUDY INDICATOR\n")
  cat(strrep("=", 80), "\n\n")

  cat("[3/10] Combining all datasets\n")

  # Bind all datasets (dplyr::bind_rows matches by column names)
  # NOTE: NSCH data excluded - see issue documentation
  calibdat <- dplyr::bind_rows(
    historical_data,
    ne25_data
    # nsch21_data,  # EXCLUDED
    # nsch22_data   # EXCLUDED
  )

  cat(sprintf("        Combined %d total records from %d studies\n",
              nrow(calibdat), length(unique(calibdat$study))))

  # Create numeric study indicator
  cat("        Creating numeric study indicator (studynum)\n")
  calibdat <- calibdat %>%
    dplyr::mutate(
      studynum = dplyr::case_when(
        study == "NE20" ~ 1,
        study == "NE22" ~ 2,
        study == "NE25" ~ 3,
        # study == "NSCH21" ~ 5,  # EXCLUDED
        # study == "NSCH22" ~ 6,  # EXCLUDED
        study == "USA24" ~ 7,
        .default = NA_real_
      )
    )

  # Get item columns (everything except metadata)
  metadata_cols <- c("study", "studynum", "id", "years")
  item_cols <- setdiff(names(calibdat), metadata_cols)

  # Filter to only include calibration items (calibration_item = true)
  cat("\n        Filtering to calibration items (calibration_item = TRUE)...\n")
  item_cols_before <- length(item_cols)
  item_cols <- intersect(item_cols, calibration_items)
  item_cols_excluded <- item_cols_before - length(item_cols)

  cat(sprintf("          Items before filtering: %d\n", item_cols_before))
  cat(sprintf("          Items after filtering: %d\n", length(item_cols)))
  cat(sprintf("          Items excluded: %d\n", item_cols_excluded))

  if (item_cols_excluded > 0) {
    excluded_items <- setdiff(setdiff(names(calibdat), metadata_cols), calibration_items)
    cat(sprintf("          Excluded items: %s\n", paste(excluded_items, collapse = ", ")))
  }

  # Sort item columns by codebook order (not alphabetically)
  # This ensures EG items come before PS items, fixing alphabetical sorting issues
  cat("\n        Sorting items by codebook order...\n")
  item_cols_sorted <- sort_items_by_codebook(item_cols, codebook, verbose = TRUE)
  cat("\n")

  # Select only metadata columns and calibration items (in codebook order)
  calibdat <- calibdat %>%
    dplyr::select(study, studynum, id, years, dplyr::all_of(item_cols_sorted))

  cat(sprintf("        Final dataset: %d records x %d columns\n",
              nrow(calibdat), ncol(calibdat)))
  cat(sprintf("          Metadata columns: %d\n", length(metadata_cols)))
  cat(sprintf("          Item columns: %d\n", length(item_cols)))

  # Report study breakdown
  cat("\n        Study record counts:\n")
  study_summary <- calibdat %>%
    dplyr::group_by(study, studynum) %>%
    dplyr::summarise(n = dplyr::n(), .groups = "drop") %>%
    dplyr::arrange(studynum)

  for (i in 1:nrow(study_summary)) {
    cat(sprintf("          %s (studynum=%d): %d records\n",
                study_summary$study[i],
                study_summary$studynum[i],
                study_summary$n[i]))
  }

  # Calculate overall missingness
  item_data <- calibdat %>% dplyr::select(dplyr::all_of(item_cols_sorted))
  total_cells <- nrow(item_data) * ncol(item_data)
  missing_cells <- sum(is.na(item_data))
  pct_missing <- (missing_cells / total_cells) * 100

  cat(sprintf("\n        Overall item missingness: %.1f%% (%d of %d cells)\n",
              pct_missing, missing_cells, total_cells))

  # Age range
  cat(sprintf("        Age range: %.2f - %.2f years (median: %.2f)\n",
              min(calibdat$years, na.rm = TRUE),
              max(calibdat$years, na.rm = TRUE),
              median(calibdat$years, na.rm = TRUE)))

  cat("\n")

  # ===========================================================================
  # Step 4: Output File Path Prompt (was Step 7)
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 4: OUTPUT FILE PATH SPECIFICATION\n")
  cat(strrep("=", 80), "\n\n")

  cat("[4/8] Specify output file path\n\n")
  cat("Calibration dataset will be written to Mplus .dat format.\n")
  cat("Default location: mplus/calibdat.dat\n\n")

  dat_file_path <- readline(prompt = "Output .dat file path (default: mplus/calibdat.dat): ")

  if (dat_file_path == "" || is.na(dat_file_path)) {
    dat_file_path <- "mplus/calibdat.dat"
    cat(sprintf("        Using default: %s\n", dat_file_path))
  } else {
    cat(sprintf("        Using custom path: %s\n", dat_file_path))
  }

  # Validate and create directory if needed
  dat_dir <- dirname(dat_file_path)
  if (!dir.exists(dat_dir)) {
    cat(sprintf("        Creating directory: %s\n", dat_dir))
    dir.create(dat_dir, recursive = TRUE, showWarnings = FALSE)
    if (!dir.exists(dat_dir)) {
      stop(sprintf("Failed to create directory: %s", dat_dir))
    }
  } else {
    cat(sprintf("        Directory exists: %s\n", dat_dir))
  }

  # Check if file already exists
  if (file.exists(dat_file_path)) {
    cat(sprintf("        [WARN] File already exists and will be overwritten: %s\n", dat_file_path))
  }

  cat("\n")

  # ===========================================================================
  # Step 5: Prepare Mplus Data & Quality Validation (was Step 8)
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 5: PREPARE MPLUS DATA & QUALITY VALIDATION\n")
  cat(strrep("=", 80), "\n\n")

  cat("[5/8] Preparing Mplus data\n")

  # Select columns for Mplus: studynum, id, years, items (alphabetically sorted)
  # Exclude "study" character column - Mplus needs numeric values only
  mplus_cols <- c("studynum", "id", "years", item_cols_sorted)
  mplus_data <- calibdat %>% dplyr::select(dplyr::all_of(mplus_cols))

  cat(sprintf("        Prepared %d records x %d columns\n",
              nrow(mplus_data), ncol(mplus_data)))
  cat(sprintf("        Columns: studynum, id, years + %d items\n", length(item_cols_sorted)))
  cat("\n")

  # -------------------------------------------------------------------------
  # Quality Validation (validate exact data going to Mplus)
  # -------------------------------------------------------------------------

  cat("        Running comprehensive quality validation on mplus_data...\n\n")

  quality_flags <- tryCatch({
    # Use mplus_data directly (not reloading from database)
    # Add study name mapping for by-study validation
    calibration_data_quality <- mplus_data %>%
      dplyr::mutate(
        study = dplyr::case_when(
          studynum == 1 ~ "NE20",
          studynum == 2 ~ "NE22",
          studynum == 3 ~ "NE25",
          studynum == 5 ~ "NSCH21",
          studynum == 6 ~ "NSCH22",
          studynum == 7 ~ "USA24",
          .default = NA_character_
        )
      )

    # Extract item columns (everything except metadata)
    metadata_cols_quality <- c("studynum", "id", "years", "study")
    item_cols_quality <- setdiff(names(calibration_data_quality), metadata_cols_quality)

    cat(sprintf("          Validating %d items across %d studies\n",
                length(item_cols_quality), length(unique(calibration_data_quality$study))))

    # Load codebook for expected categories and instruments
    codebook_quality <- jsonlite::fromJSON(codebook_path, simplifyVector = FALSE)

    # Extract expected categories from codebook
    expected_categories <- list()
    for (item_id in names(codebook_quality$items)) {
      item <- codebook_quality$items[[item_id]]
      if (!is.null(item$lexicons$equate)) {
        equate_name <- item$lexicons$equate

        # Get response set reference
        response_ref <- NULL
        if (!is.null(item$content$response_options$ne25)) {
          response_ref <- item$content$response_options$ne25
        } else if (!is.null(item$content$response_options$ne22)) {
          response_ref <- item$content$response_options$ne22
        } else if (!is.null(item$content$response_options$ne20)) {
          response_ref <- item$content$response_options$ne20
        }

        # Extract values from response_sets
        if (!is.null(response_ref) &&
            length(response_ref) == 1 &&
            is.character(response_ref) &&
            response_ref %in% names(codebook_quality$response_sets)) {

          response_set <- codebook_quality$response_sets[[response_ref]]

          # Extract non-missing numeric values
          non_missing_values <- sapply(seq_along(response_set), function(i) {
            opt <- response_set[[i]]
            is_missing <- !is.null(opt$missing) && opt$missing == TRUE
            if (!is_missing) {
              return(as.numeric(opt$value))
            } else {
              return(NA)
            }
          })

          non_missing_values <- non_missing_values[!is.na(non_missing_values)]
          expected_categories[[equate_name]] <- sort(non_missing_values)
        }
      }
    }

    # Extract instrument mapping from codebook
    instrument_lookup <- list()
    for (item_id in names(codebook_quality$items)) {
      item <- codebook_quality$items[[item_id]]
      if (!is.null(item$lexicons$equate)) {
        equate_name <- item$lexicons$equate
        if (!is.null(item$instruments) && length(item$instruments) > 0) {
          instrument_lookup[[equate_name]] <- unlist(item$instruments)
        }
      }
    }

    # Perform quality checks
    cat("          Quality checks:\n")
    cat("            [1/3] Category mismatch detection\n")
    cat("            [2/3] Age-response correlation check (Kidsights items only)\n")
    cat("            [3/3] Non-sequential values check\n\n")

    # Initialize flags data frame
    flags <- data.frame(
      item_id = character(),
      study = character(),
      flag_type = character(),
      flag_severity = character(),
      observed_categories = character(),
      expected_categories = character(),
      correlation_value = numeric(),
      n_responses = integer(),
      pct_missing = numeric(),
      description = character(),
      stringsAsFactors = FALSE
    )

    # FLAG 1: Category mismatch detection
    flag1_count <- 0
    for (item in item_cols_quality) {
      if (!item %in% names(expected_categories)) {
        next
      }

      expected_vals <- expected_categories[[item]]

      for (study_name in unique(calibration_data_quality$study)) {
        study_data <- calibration_data_quality %>%
          dplyr::filter(study == study_name) %>%
          dplyr::pull(!!item)

        study_data_clean <- study_data[!is.na(study_data)]

        if (length(study_data_clean) == 0) {
          next
        }

        observed_vals <- sort(unique(study_data_clean))

        fewer_categories <- all(observed_vals %in% expected_vals) && length(observed_vals) < length(expected_vals)
        different_categories <- !all(observed_vals %in% expected_vals)

        if (fewer_categories || different_categories) {
          flag_type_detail <- if (different_categories) {
            "CATEGORY_MISMATCH_INVALID"
          } else {
            "CATEGORY_MISMATCH_FEWER"
          }

          flag_severity <- if (different_categories) "ERROR" else "WARNING"

          description <- if (different_categories) {
            sprintf("Invalid values detected: %s (expected: %s)",
                    paste(setdiff(observed_vals, expected_vals), collapse = ","),
                    paste(expected_vals, collapse = ","))
          } else {
            sprintf("Fewer categories observed (%d/%d): %s (expected: %s)",
                    length(observed_vals), length(expected_vals),
                    paste(observed_vals, collapse = ","),
                    paste(expected_vals, collapse = ","))
          }

          flags <- rbind(flags, data.frame(
            item_id = item,
            study = study_name,
            flag_type = flag_type_detail,
            flag_severity = flag_severity,
            observed_categories = paste(observed_vals, collapse = ","),
            expected_categories = paste(expected_vals, collapse = ","),
            correlation_value = NA,
            n_responses = length(study_data_clean),
            pct_missing = sum(is.na(study_data)) / length(study_data) * 100,
            description = description,
            stringsAsFactors = FALSE
          ))

          flag1_count <- flag1_count + 1
        }
      }
    }

    # FLAG 2: Negative age-response correlation (Kidsights items only)
    flag2_count <- 0
    for (item in item_cols_quality) {
      # Only check age-gradient for Kidsights Measurement Tool items
      item_instruments <- instrument_lookup[[item]]

      if (is.null(item_instruments) ||
          !("Kidsights Measurement Tool" %in% item_instruments)) {
        next
      }

      for (study_name in unique(calibration_data_quality$study)) {
        study_data <- calibration_data_quality %>%
          dplyr::filter(study == study_name) %>%
          dplyr::select(years, !!item)

        study_data_clean <- study_data[!is.na(study_data[[item]]), ]

        if (nrow(study_data_clean) < 10) {
          next
        }

        cor_value <- cor(study_data_clean$years, study_data_clean[[item]], method = "pearson")

        if (!is.na(cor_value) && cor_value < 0) {
          flags <- rbind(flags, data.frame(
            item_id = item,
            study = study_name,
            flag_type = "NEGATIVE_CORRELATION",
            flag_severity = "WARNING",
            observed_categories = NA,
            expected_categories = NA,
            correlation_value = cor_value,
            n_responses = nrow(study_data_clean),
            pct_missing = sum(is.na(study_data[[item]])) / nrow(study_data) * 100,
            description = sprintf("Negative correlation (r = %.3f): Older children score lower", cor_value),
            stringsAsFactors = FALSE
          ))

          flag2_count <- flag2_count + 1
        }
      }
    }

    # FLAG 3: Non-sequential response values
    flag3_count <- 0
    for (item in item_cols_quality) {
      for (study_name in unique(calibration_data_quality$study)) {
        study_data <- calibration_data_quality %>%
          dplyr::filter(study == study_name) %>%
          dplyr::pull(!!item)

        study_data_clean <- study_data[!is.na(study_data)]

        if (length(study_data_clean) == 0) {
          next
        }

        observed_vals <- sort(unique(study_data_clean))

        if (length(observed_vals) > 1) {
          diffs <- diff(observed_vals)
          is_sequential <- all(diffs == 1)

          if (!is_sequential) {
            flags <- rbind(flags, data.frame(
              item_id = item,
              study = study_name,
              flag_type = "NON_SEQUENTIAL",
              flag_severity = "WARNING",
              observed_categories = paste(observed_vals, collapse = ","),
              expected_categories = NA,
              correlation_value = NA,
              n_responses = length(study_data_clean),
              pct_missing = sum(is.na(study_data)) / length(study_data) * 100,
              description = sprintf("Non-sequential values: %s (gaps: %s)",
                                    paste(observed_vals, collapse = ","),
                                    paste(diffs, collapse = ",")),
              stringsAsFactors = FALSE
            ))

            flag3_count <- flag3_count + 1
          }
        }
      }
    }

    # Write quality flags to CSV
    quality_output_path <- "docs/irt_scoring/quality_flags.csv"

    if (nrow(flags) > 0) {
      if (!dir.exists(dirname(quality_output_path))) {
        dir.create(dirname(quality_output_path), recursive = TRUE)
      }
      write.csv(flags, quality_output_path, row.names = FALSE)
      cat(sprintf("          [WARN] Found %d quality issues:\n", nrow(flags)))
      cat(sprintf("                 - Category mismatches: %d\n", flag1_count))
      cat(sprintf("                 - Negative correlations: %d\n", flag2_count))
      cat(sprintf("                 - Non-sequential values: %d\n", flag3_count))
      cat(sprintf("          Quality flags: %s\n", quality_output_path))
    } else {
      cat("          [OK] No quality issues detected\n")
    }

    # Generate cross-tabulation: items × response values
    cat("          Generating item response cross-tabulation...\n")

    # Build cross-tabulation data frame
    crosstab_list <- list()

    for (item_name in item_cols_quality) {
      values <- calibration_data_quality[[item_name]]
      value_counts <- table(values, useNA = "ifany")

      # Create row for this item
      row_data <- list(item = item_name)

      # Add counts for each observed value
      for (val_name in names(value_counts)) {
        col_name <- if (is.na(val_name)) "NA" else paste0("y", val_name)
        row_data[[col_name]] <- as.integer(value_counts[val_name])
      }

      crosstab_list[[item_name]] <- row_data
    }

    # Convert to data frame
    crosstab_df <- dplyr::bind_rows(crosstab_list)

    # Replace NA with 0 for counts
    crosstab_df[is.na(crosstab_df)] <- 0

    # Write cross-tabulation to CSV
    crosstab_output_path <- "docs/irt_scoring/item_response_crosstab.csv"

    if (!dir.exists(dirname(crosstab_output_path))) {
      dir.create(dirname(crosstab_output_path), recursive = TRUE)
    }

    write.csv(crosstab_df, crosstab_output_path, row.names = FALSE)

    cat(sprintf("          Cross-tabulation: %s\n", crosstab_output_path))
    cat(sprintf("          Items: %d, Response columns: %d\n", nrow(crosstab_df), ncol(crosstab_df) - 1))

    flags

  }, error = function(e) {
    cat(sprintf("          [WARN] Quality validation failed: %s\n", e$message))
    cat("          Continuing without quality report\n")
    data.frame()
  })

  cat("\n")

  # -------------------------------------------------------------------------
  # Write Mplus Files
  # -------------------------------------------------------------------------

  cat("        Writing Mplus .dat and .inp files...\n")
  cat(sprintf("        Format: tab-delimited, missing = '.'\n"))

  # Use MplusAutomation to create both .dat and .inp files
  # This ensures:
  #   - .dat file: tab-delimited, missing as ".", no headers
  #   - .inp file: automatically generated with correct NAMES field
  #   - Perfect synchronization between .dat columns and .inp NAMES
  MplusAutomation::prepareMplusData(
    df = mplus_data,
    filename = dat_file_path,
    inpfile = TRUE,
    quiet = FALSE
  )

  # Check files were created
  if (!file.exists(dat_file_path)) {
    stop(sprintf("Failed to create Mplus .dat file: %s", dat_file_path))
  }

  # Get .inp filename (same name but .inp extension)
  inp_file_path <- sub("\\.dat$", ".inp", dat_file_path)

  if (!file.exists(inp_file_path)) {
    cat(sprintf("        [WARN] .inp file not created: %s\n", inp_file_path))
  }

  # Report file sizes
  file_size_bytes <- file.info(dat_file_path)$size
  file_size_mb <- file_size_bytes / (1024^2)

  cat(sprintf("        [OK] .dat file written: %s\n", dat_file_path))
  cat(sprintf("        File size: %.2f MB (%s bytes)\n",
              file_size_mb, format(file_size_bytes, big.mark = ",")))

  if (file.exists(inp_file_path)) {
    inp_size_bytes <- file.info(inp_file_path)$size
    inp_size_kb <- inp_size_bytes / 1024
    cat(sprintf("        [OK] .inp file written: %s\n", inp_file_path))
    cat(sprintf("        File size: %.2f KB (%s bytes)\n",
                inp_size_kb, format(inp_size_bytes, big.mark = ",")))
  }

  cat("\n")

  # ===========================================================================
  # Step 6: Insert into DuckDB (was Step 9)
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 6: INSERT INTO DUCKDB\n")
  cat(strrep("=", 80), "\n\n")

  cat("[6/8] Creating DuckDB table: calibration_dataset_2020_2025\n")

  # Connect to DuckDB (read_only = FALSE for writing)
  conn <- duckdb::dbConnect(duckdb::duckdb(), dbdir = db_path, read_only = FALSE)

  table_name <- "calibration_dataset_2020_2025"

  # Drop table if exists
  if (table_name %in% DBI::dbListTables(conn)) {
    cat(sprintf("        Dropping existing table: %s\n", table_name))
    DBI::dbExecute(conn, sprintf("DROP TABLE %s", table_name))
  }

  # Insert calibration dataset
  cat(sprintf("        Inserting %d records into %s\n", nrow(calibdat), table_name))
  DBI::dbWriteTable(conn, table_name, calibdat, overwrite = TRUE)

  # Create indexes for faster querying
  cat("        Creating indexes:\n")

  index_queries <- c(
    sprintf("CREATE INDEX idx_%s_study ON %s (study)", table_name, table_name),
    sprintf("CREATE INDEX idx_%s_studynum ON %s (studynum)", table_name, table_name),
    sprintf("CREATE INDEX idx_%s_id ON %s (id)", table_name, table_name),
    sprintf("CREATE INDEX idx_%s_study_id ON %s (study, id)", table_name, table_name)
  )

  for (query in index_queries) {
    tryCatch({
      DBI::dbExecute(conn, query)
      index_name <- sub("CREATE INDEX (\\w+) .*", "\\1", query)
      cat(sprintf("          [OK] %s\n", index_name))
    }, error = function(e) {
      cat(sprintf("          [WARN] Index creation failed: %s\n", e$message))
    })
  }

  # Verify record count
  db_count <- DBI::dbGetQuery(conn,
    sprintf("SELECT COUNT(*) as n FROM %s", table_name))$n

  cat(sprintf("\n        Verification: %d records in database\n", db_count))

  if (db_count != nrow(calibdat)) {
    cat(sprintf("        [WARN] Record count mismatch! Expected: %d, Found: %d\n",
                nrow(calibdat), db_count))
  } else {
    cat("        [OK] Record count matches\n")
  }

  # Disconnect
  DBI::dbDisconnect(conn)
  cat("        Disconnected from database\n")

  cat("\n")

  # ===========================================================================
  # Step 7: Summary Report (was Step 10)
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 7: SUMMARY REPORT\n")
  cat(strrep("=", 80), "\n\n")

  cat("[7/8] Generating summary report\n\n")

  # Final record counts by study
  cat("Study Record Counts:\n")
  cat(strrep("-", 80), "\n")
  study_final <- calibdat %>%
    dplyr::group_by(study, studynum) %>%
    dplyr::summarise(
      n = dplyr::n(),
      pct = (dplyr::n() / nrow(calibdat)) * 100,
      .groups = "drop"
    ) %>%
    dplyr::arrange(studynum)

  total_records <- sum(study_final$n)

  for (i in 1:nrow(study_final)) {
    cat(sprintf("  %6s (studynum=%d): %6d records (%5.1f%%)\n",
                study_final$study[i],
                study_final$studynum[i],
                study_final$n[i],
                study_final$pct[i]))
  }
  cat(sprintf("  %6s              %6d records (100.0%%)\n", "TOTAL", total_records))

  cat("\n")
  cat("  [NOTE] NSCH 2021 and NSCH 2022 data EXCLUDED due to quality concerns.\n")
  cat("         See: .github/ISSUE_TEMPLATE/nsch_calibration_data_exclusion.md\n")

  # Item coverage summary
  cat("\n")
  cat("Item Coverage:\n")
  cat(strrep("-", 80), "\n")
  cat(sprintf("  Total items: %d\n", length(item_cols_sorted)))
  cat(sprintf("  Age range: %.2f - %.2f years (median: %.2f)\n",
              min(calibdat$years, na.rm = TRUE),
              max(calibdat$years, na.rm = TRUE),
              median(calibdat$years, na.rm = TRUE)))

  # Calculate item missingness statistics
  item_data_final <- calibdat %>% dplyr::select(dplyr::all_of(item_cols_sorted))
  item_missingness <- sapply(item_data_final, function(x) sum(is.na(x)) / length(x) * 100)

  cat(sprintf("  Item missingness: %.1f%% - %.1f%% (median: %.1f%%)\n",
              min(item_missingness),
              max(item_missingness),
              median(item_missingness)))

  # Count items with complete coverage (0% missing)
  n_complete <- sum(item_missingness == 0)
  cat(sprintf("  Items with complete data: %d of %d (%.1f%%)\n",
              n_complete, length(item_cols_sorted),
              (n_complete / length(item_cols_sorted)) * 100))

  # Output file paths
  cat("\n")
  cat("Output Files:\n")
  cat(strrep("-", 80), "\n")
  cat(sprintf("  Mplus .dat file: %s\n", dat_file_path))
  cat(sprintf("    Size: %.2f MB\n", file_size_mb))
  cat(sprintf("    Format: %d records x %d columns (tab-delimited, missing='.')\n",
              nrow(mplus_data), ncol(mplus_data)))

  # Report .inp file if it was created
  inp_file_path <- sub("\\.dat$", ".inp", dat_file_path)
  if (file.exists(inp_file_path)) {
    inp_size_bytes <- file.info(inp_file_path)$size
    inp_size_kb <- inp_size_bytes / 1024
    cat(sprintf("\n  Mplus .inp file: %s\n", inp_file_path))
    cat(sprintf("    Size: %.2f KB\n", inp_size_kb))
    cat(sprintf("    Generated by: MplusAutomation::prepareMplusData()\n"))
    cat(sprintf("    NAMES field: Matches .dat columns exactly\n"))
  }

  cat(sprintf("\n  DuckDB table: %s\n", table_name))
  cat(sprintf("    Location: %s\n", db_path))
  cat(sprintf("    Records: %d\n", db_count))
  cat(sprintf("    Indexes: study, studynum, id, (study,id)\n"))

  # Next steps
  cat("\n")
  cat("Next Steps for Mplus IRT Calibration:\n")
  cat(strrep("-", 80), "\n")
  cat("  1. Edit Mplus input file (calibdat.inp):\n")
  cat(sprintf("     - Update TITLE section\n"))
  cat("     - Define IRT model structure in MODEL section (graded response model recommended)\n")
  cat("     - NAMES field already generated correctly by MplusAutomation\n")
  cat("\n  2. Run Mplus calibration:\n")
  cat("     - Estimate item parameters across all studies\n")
  cat("     - Use studynum as grouping variable if needed\n")
  cat("     - Check for differential item functioning (DIF)\n")
  cat("\n  3. Extract calibrated parameters:\n")
  cat("     - Item discrimination (slope) parameters\n")
  cat("     - Item threshold (difficulty) parameters\n")
  cat("     - Store in codebook IRT parameters section\n")
  cat("\n  4. Score NE25 data using calibrated parameters:\n")
  cat("     - Apply IRT scoring to ne25_transformed items\n")
  cat("     - Generate theta scores for each domain\n")
  cat("     - Update ne25_scored table\n")

  cat("\n")

  # ===========================================================================
  # Step 8: Generate Mplus MODEL Syntax (was Step 10)
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("STEP 8: GENERATE MPLUS MODEL SYNTAX\n")
  cat(strrep("=", 80), "\n\n")

  cat("[8/8] Generating Mplus MODEL, CONSTRAINT, and PRIOR syntax...\n\n")

  # Source syntax generation functions
  cat("Loading syntax generation functions...\n")
  source("scripts/irt_scoring/calibration/write_syntax2.R")
  source("scripts/irt_scoring/calibration/helpers/build_equate_table.R")

  # Generate syntax and write to Excel file
  syntax_output_path <- "mplus/kidsights_syntax_FIXED.xlsx"

  cat(sprintf("Output file: %s\n\n", syntax_output_path))

  # Generate syntax
  source("scripts/irt_scoring/calibration/generate_model_syntax.R")

  syntax_result <- generate_kidsights_model_syntax(
    scale_name = "kidsights",
    codebook_path = codebook_path,
    db_path = db_path,
    calibration_table = table_name,
    output_xlsx = syntax_output_path,
    output_inp = NULL,  # Don't generate .inp (already have calibdat.inp)
    apply_1pl = FALSE,
    verbose = TRUE
  )

  cat("\n")
  cat("[OK] Mplus syntax generation complete\n")
  cat(sprintf("     Excel file: %s\n", syntax_output_path))
  cat("     Sheets: MODEL, CONSTRAINT, PRIOR\n\n")

  # ===========================================================================
  # Completion
  # ===========================================================================

  cat(strrep("=", 80), "\n")
  cat("CALIBRATION DATASET PREPARATION COMPLETE\n")
  cat(strrep("=", 80), "\n\n")

  cat("[OK] Calibration dataset ready for Mplus IRT recalibration\n")
  cat(sprintf("[OK] Quality validation: %d flags detected\n", nrow(quality_flags)))
  cat("\n")

  return(invisible(NULL))
}

# =============================================================================
# Execute if run as script (not sourced)
# =============================================================================

if (!interactive()) {
  prepare_calibration_dataset()
}
